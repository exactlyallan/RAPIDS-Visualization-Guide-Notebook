{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fe9f3f-dfa3-46de-a733-28e429699f07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "22337288-915b-4ab8-b6f0-b07baff8a887",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/jupytercon/2020-exactlyallan/raw/master/images/RAPIDS-header-graphic.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae1471f-89d0-4d53-92ed-2e243295348e",
   "metadata": {},
   "source": [
    "# Using RAPIDS and Jupyter to Accelerate Visualization Workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0895638-518f-42c8-9885-4ea5be537f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run this cell to play the walk through video: ##\n",
    "from IPython.display.IFrame import HTML\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/TnN3a-G_ugs\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63f7e0e-9864-4b43-ab82-12a038c9604f",
   "metadata": {},
   "source": [
    "## Introduction to RAPIDS\n",
    "Backed by NVIDIA, the **[RAPIDS](https://rapids.ai/index.html)** suite of open source software libraries gives you the ability to execute end-to-end data science and analytics pipelines entirely on GPUs.\n",
    "\n",
    "Some of the main libraries includes [**cuDF**](https://docs.rapids.ai/api/cudf/stable/), a pandas-like dataframe manipulation library; [**cuML**](https://docs.rapids.ai/api/cuml/stable/), a collection of machine learning libraries that provide GPU versions of algorithms available in scikit-learn; [**cuGraph**](https://docs.rapids.ai/api/cugraph/stable/), a NetworkX-like accelerated graph analytics library; and [**cuSpatial**](https://docs.rapids.ai/api/cuspatial/stable/), a library for common spatial and spatiotemporal operations.\n",
    "\n",
    "For more general information, check out the **[RAPIDS.ai home page](https://rapids.ai/index.html)**.\n",
    "\n",
    "For a detailed presentation about RAPIDS and the latest release notes, visit the **[RAPIDS overview documentation](https://docs.rapids.ai/overview)**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef246756-7006-4cdb-a5f7-884e73671b12",
   "metadata": {},
   "source": [
    "## Introduction to RAPIDS Visualization\n",
    "The RAPIDS viz group's overall goal is to build open source libraries and collaborate with other open source projects. We hope to foster a greater adoption of GPUs in the python visualization ecosystem and beyond. Its not just for the sake of making things faster - we feel that when data scientist and analysts are able to interact with larger datasets in real time and with high fidelity, they will be able to ask better questions, more often, and get more accurate answers to today's complex problems.\n",
    "\n",
    "\n",
    "## RAPIDS Supported Viz Frameworks\n",
    "The below frameworks currently support RAPIDS - primarily through using cuDF as a data source: \n",
    "\n",
    "- **[hvplot](https://hvplot.holoviz.org/)**: wrapper API for easily visualizing data. \n",
    "- **[cuxfilter](https://github.com/rapidsai/cuxfilter)**: RAPIDS library for easily cross-filtering data. \n",
    "- **[Plotly Dash](https://plotly.com/dash/gpu-dask-acceleration/)**: framework for production ready visualization applications.\n",
    "- **[Datashader](https://datashader.org/)**: library for high fidelity server side data rendering.\n",
    "\n",
    "The RAPIDS visualization team is continually working to integrate with other open source projects - if you wish to help or have questions, reach out on our [Community Slack Channel (GOAI)](https://join.slack.com/t/rapids-goai/shared_invite/zt-h54mq1uv-KHeHDVCYs8xvZO5AB~ctTQ). \n",
    "\n",
    "### GPU Compute and/or GPU Render\n",
    "Generally RAPIDS works to accelerate visualization through faster compute - that is computing aggregations, filters, algorithms etc. quickly enough to be directly interacted with through a visualization. GPUs can also speed up visualization through faster data rendering (of which people more often associate GPUs). The architecture required to do one or both of these through web browsers can be complex, but is useful to understand when building advanced visualizations. Feel free to ask for details and future plans in our [Community Slack Channel (GOAI)](https://join.slack.com/t/rapids-goai/shared_invite/zt-h54mq1uv-KHeHDVCYs8xvZO5AB~ctTQ).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de910fb5-c704-43d5-9f9d-76f7f428fea6",
   "metadata": {},
   "source": [
    "## Hardware and Software Requirements\n",
    "To run RAPIDS you will need to meet these general requirements:\n",
    "- NVIDIA Pascal™ or better GPU\n",
    "- Ubuntu 16.04+ or CentOS 7 OS (Windows support pending)\n",
    "- Recent CUDA & NVIDIA Drivers\n",
    "- Docker and/or Anaconda\n",
    "\n",
    "For the most up to date requirements and installation details, see the [RAPIDS Getting Started Page](https://rapids.ai/start.html).\n",
    "\n",
    "### Package Requirments\n",
    "Other packages are required in addition to a RAPIDS (0.16+) release installation. Everything is listed in the `environment.yml` and can be installed via [conda forge](https://conda-forge.org/). Using `conda`, first execute:\n",
    "```\n",
    "conda env create --name jupytercon_tutorial --file environment.yml\n",
    "```\n",
    "Then:\n",
    "```\n",
    "conda activate jupytercon_tutorial\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4140b683-3ba7-41c5-8670-fd2b2846c613",
   "metadata": {},
   "source": [
    "# Index of Notebooks\n",
    "\n",
    "- 00 **Index**: you are here (but are we anywhere..really?)\n",
    "- [01 **Data inspection and validation**](01%20Data%20Inspection%20and%20Validation.ipynb): dataset procurement as well as inspection with hvplot via bokeh charts.\n",
    "- [02 **Exploratory data visualization**](02%20Exploratory%20Data%20Visualization.ipynb): exploring preliminary patterns through cross-filtering with cuxfilter.\n",
    "- [03 **Data analysis with visual analytics**](03%20Data%20Analysis%20with%20Visual%20Analytics.ipynb): applying visual analytics with cuSpatial, cuGraph, hvplot via bokeh charts and datashader. \n",
    "- [04 **Explanatory data visualization**](04%20Explanatory%20Data%20Visualization.ipynb): presenting findings through a visualization application with Plotly Dash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9564f55d-df3c-4e19-a6be-335b40ae58c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f003a8e5-16c9-4b8b-926d-7aeb1d9678bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e82918b5-2e3c-419d-8992-a5fa9416e73c",
   "metadata": {},
   "source": [
    "# Data Inspection and Validation\n",
    "***Loading data, vetting its quality, and understanding its shape***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2ee0b9-6daa-43b3-a629-932826087b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run this cell to show the next section's walkthrough video ##\n",
    "from IPython.display import HTML\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/0PNdgpZGPuk\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556d6721-eb1a-4623-bb03-efe7d7027823",
   "metadata": {},
   "source": [
    "## Overview\n",
    "This intro notebook will use cuDF and hvplot (with bokeh charts) to load a public bike share dataset and get a general sense of what it contains, then run some cursory visualization to validate that the data is free of issues.\n",
    "\n",
    "### cuDF and hvplot\n",
    "- [cuDF](https://docs.rapids.ai/api/cudf/stable/), the core of RAPIDS, is a Python GPU DataFrame library (built on the Apache Arrow columnar memory format) for loading, joining, aggregating, filtering, and otherwise manipulating data in a pandas-like API.\n",
    "- [hvplot](https://hvplot.holoviz.org/) is a high-level plotting API for the PyData ecosystem built on [HoloViews](http://holoviews.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1d6834-a273-4529-b595-76b1f45a0d8d",
   "metadata": {},
   "source": [
    "## Imports\n",
    "Let's first make sure the necessary imports are present to load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553aef87-f70a-4859-85c5-0e0447dfbde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "import hvplot.cudf\n",
    "import cupy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35c4833-39a8-42ea-b55e-c0ee692cd70a",
   "metadata": {},
   "source": [
    "## Data Size and GPU Speedups\n",
    "This tutorial's dataset size is about `2.1GB` unzipped and contains about `9 million rows`. While this will do for a tutorial, its still too small to get a sense of the speed up possible with GPU acceleration. We've created a larger `300 million row` [2010 Census Visualization](https://github.com/rapidsai/plotly-dash-rapids-census-demo) application available through the RAPIDS [GitHub page](https://github.com/rapidsai) as another demo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e9d7d6-6f12-4c7a-965f-41ee2ee184b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run this cell to show the next section's walkthrough video ##\n",
    "from IPython.display import HTML\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Q6UQullAAvY\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b731b02c-04ce-4e20-9a17-2944bdbe9deb",
   "metadata": {},
   "source": [
    "## Loading Data into cuDF\n",
    "We need to download and extract the sample data we will use for this tutorial. This notebook uses the Kaggle [Chicago Divvy Bicycle Sharing Data](https://www.kaggle.com/yingwurenjian/chicago-divvy-bicycle-sharing-data) dataset. Once the `data.csv` file is downloaded and unzipped, point the paths below at the location *(Make sure to set DATA_DIR to the path you saved that data file to)*:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a029228d-0802-4ea9-b37a-4da55f084c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = Path(\"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2008b9-e2ff-4e12-89cd-66d42cd97ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and Extract the dataset\n",
    "! wget -N -P {DATA_DIR} https://data.rapids.ai/viz-data/data.tar.xz\n",
    "! tar -xf {DATA_DIR}/data.tar.xz -C {DATA_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cb2df5-732e-45f7-8a64-0eb87fe66342",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILENAME = Path(\"data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79ec78c-2d86-4511-8737-875fd1394507",
   "metadata": {},
   "source": [
    "We now read the .csv file into the GPU cuDF Dataframe (which behaves similar to a Pandas dataframe). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7681115-ea4d-45a3-b498-37de95ba14b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = cudf.read_csv(DATA_DIR / FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d7422e-b7c7-43c9-8bb9-9018f8f6c95e",
   "metadata": {},
   "source": [
    "## Mapping out the Data Shape\n",
    "CuDF supports all the standard Pandas operations for a quick look at the data e.g. to see the total number of rows..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97be73e5-a3f2-4693-9975-d6ec01a68258",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380db311-d57e-4dc1-b204-c2879ada8a57",
   "metadata": {},
   "source": [
    "Or to inspect the column headers and first few rows..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0137c809-fdbc-4f35-9ef2-d16c13f8c976",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723237cd-1e8e-4cc2-867b-ed5c45abdb33",
   "metadata": {},
   "source": [
    "Or to see the full list of columns..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e84b014-aff7-4356-8ede-1d142e589252",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac0aa52-a3be-472f-abdb-1190ba0e7e23",
   "metadata": {},
   "source": [
    "Or see how many trips were made by subscribers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26404277-ce5a-409b-a1b0-063274634fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"usertype\").size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1894eb6c-1de9-47e8-8538-07b317cf8494",
   "metadata": {},
   "source": [
    "## Improving Data Utility\n",
    "Now that we have a basic idea of how big our dataset is and what it contains, we want to start making the data more meaningful. This task can vary from removing unnecessary columns, mapping values to be more human readable, or formatting them to be understood by our tools.  \n",
    "\n",
    "Having looked at the `df.head()` above, the first thing we might want is to re-load the data, parsing the start-stop time columns as more usable datetimes types: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeae6ebc-f607-4d24-9488-852efe1bdd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = cudf.read_csv(DATA_DIR / FILENAME, parse_dates=('starttime', 'stoptime'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a784621-279d-499c-8f22-126ed8999830",
   "metadata": {},
   "source": [
    "One thing we will want to do is to look at trips by day of week. Now that we have real datetime columns, we can use `dt.weekday` to add a `weekday` column to our `cudf` Dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c27c96e-b33d-4eaf-be90-c141574870fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"weekday\"] = df['starttime'].dt.weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ab25ca-92f3-426d-95ce-a0549a18025b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run this cell to show the next section's walkthrough video ##\n",
    "from IPython.display import HTML\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/2BrOrIRp76M\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d14364-57aa-475b-bf0e-1946e736db9b",
   "metadata": {},
   "source": [
    "## Inspecting Data Quality and Distribution\n",
    "Another important step is getting a sense of the quality of the dataset. As these datasets are often larger than is feasible to look through row by row, mapping out the distribution of values early on helps find issuse that can derail an analysis later.\n",
    "\n",
    "Some examples are gaps in data, unexpected or empty value types, infeasible values, or incorrect projections. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cbdb66-e786-45ec-a30b-0c85a44fdb67",
   "metadata": {},
   "source": [
    "## Gender and Subsriber Columns\n",
    "We could do this in a numerical way, such as getting the totals from the 'gender' data column as a table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd825789-2069-4dc6-8363-7c4e17ee83bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mf_counts = df.groupby(\"gender\").size().rename(\"count\").reset_index()\n",
    "mf_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c0c803-e2c6-4f2f-8691-b729f6339304",
   "metadata": {},
   "source": [
    "While technically functional as a table, taking values and visualizating them as bars help to intuitively show the scale of the difference faster (hvplot's API makes this very simple):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc64cd1-f003-4744-9756-60dd8f84e059",
   "metadata": {},
   "outputs": [],
   "source": [
    "mf_counts.hvplot.bar(\"gender\",\"count\").opts(title=\"Total trips by gender\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0410291-9e5a-467e-a98b-25b2efebb594",
   "metadata": {},
   "source": [
    "### A Note on Preattentive Attributes\n",
    "This subconcious ability to quickly recognize patterns is due to our brain's natural ability to find [preattentive attributes](http://daydreamingnumbers.com/blog/preattentive-attributes-example/), such as height, orientation, or color. Imagine 100 values in a table and 100 in a bar chart and how quickly you would be albe to find the smallest and largest values in either."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8231b39e-96e2-43fe-ab9f-37050cc37b5b",
   "metadata": {},
   "source": [
    "### Try It out\n",
    "Now try using [hvplot's user guide](https://hvplot.holoviz.org/user_guide/Plotting.html) and our examples to create a hvplot that shows the distribution of `Subscriber` types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f60e20-b67e-471d-837f-44bc7ee8cb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52995a8d-95c7-4f2e-83d9-714252d4ea12",
   "metadata": {},
   "source": [
    "The above data columns maybe show some potentially useful disparities, but without supplimental data, it would be hard to have a follow up question.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a80057-8495-4ee3-b0ce-ec6f9ee7784d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run this cell to show the next section's walkthrough video ##\n",
    "from IPython.display import HTML\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/fRH03WEsyVk\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00689d9d-86f9-4a1c-8d0c-a275ca6509c7",
   "metadata": {},
   "source": [
    "## Trip Starts\n",
    "Instead, another question we might want to ask is how many trip starts are there per day of the week? We can group the `cudf` Dataframe and call `hvplot.bar` directly the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2081845d-6096-4ae2-a042-7cbab90bcd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "day_counts = df.groupby(\"weekday\").size().rename(\"count\").reset_index()\n",
    "day_counts.hvplot.bar(\"weekday\", \"count\").opts(title=\"Trip starts, per Week Day\", yformatter=\"%0.0f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e9ec41-a0dd-4ef1-a994-a86838c611d5",
   "metadata": {},
   "source": [
    "With 0-4 being a weekday, and 5-6 being a weekend, there is a clear drop off of ridership on the weekends. Lets note that!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f04f570-7a27-419e-9e7e-7f771d22c68d",
   "metadata": {},
   "source": [
    "## Trips by Duration\n",
    "Another quick look we can generate is to see the overall distribution of trip durations, this time using `hvplot.hist`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd94ed53-cd24-4b99-8e20-22932aaac32e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We selected an arbitrary 50 for bin size, try and see patterns with other sizes\n",
    "df.hvplot.hist(y=\"tripduration\").opts(\n",
    "    title=\"Trips Duration Histrogram\", yformatter=\"%0.0f\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5b0497-1d84-4897-8873-5fba610f427b",
   "metadata": {},
   "source": [
    "Clearly, most trips are less than 15 minuites long. \n",
    "\n",
    "`hvplot` also makes it simple to interrogate different dimensions. For example, we can add `groupby=\"month\"` to our call to `hvplot.hist`, and automatically get a slider to see a histogram specific to each month:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661eda58-a57c-40c2-bff3-8de71f84ce38",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hvplot.hist(y=\"tripduration\", bins=50, groupby=\"month\").opts(\n",
    "    title=\"Trips Duration Histrogram by Month\", yformatter=\"%0.0f\", width=400\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee76608-fd7d-4a02-bc6d-b84576e48aaf",
   "metadata": {},
   "source": [
    "By scrubbing between the months we can start to see a pattern of slightly longer trip durations emerge during the summer months.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4736f83-b02b-4fdf-b053-0a3c94938317",
   "metadata": {},
   "source": [
    "## Trips vs Temperatures\n",
    "Lets follow up on this by using `hvplot` to generate a KDE distributions using our `cudf` Dataframes for 9 million trips:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a559794-bbb1-4bf0-882d-6c0a2dde5f64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.hvplot.kde(y=\"temperature\").opts(title=\"Distribution of trip temperatures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6e0b95-9ba6-4f47-9c78-a0e1533aaaf3",
   "metadata": {},
   "source": [
    "Clearly most trips occur around a temperature sweet spot of around 65-80 degrees.\n",
    "\n",
    "\n",
    "The `hvplot.heatmap` method can group in two dimensions and colormap according to aggregations on those groups. Here we see *average* trip duration by year and month: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3cafa0-1349-42b3-9c7a-6a090c2196e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hvplot.heatmap(x='month', y='year', C='tripduration', \n",
    "                  reduce_function=cudf.DataFrame.mean , colorbar=True, cmap=\"Viridis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01fd541-0e09-4bf0-a1fe-715f04964d25",
   "metadata": {},
   "source": [
    "So what we saw hinted at with the trip duration slider is much more clearly shown in this literal heatmap *(ba-dom-tss)*. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104cd4f8-f22d-4f15-8aba-a7f8f944cb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run this cell to show the next section's walkthrough video ##\n",
    "from IPython.display import HTML\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/gqkdgOKiGNM\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e0521b-a7f4-4553-85f7-f04dfe9c2482",
   "metadata": {},
   "source": [
    "## Trip Geography\n",
    "Temperature and months aside, we might also want to bin the data geographically to check for anomalies. The `hvplot.hexbin` can show the counts for trip starts overlaid on a tile map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2dd1eb-5100-41be-9a64-40182b8ddcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hvplot.hexbin(x='longitude_start', y='latitude_start', geo=True, tiles=\"OSM\").opts(width=600, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffe93f0-e609-4a8d-ad81-288f9577a97f",
   "metadata": {},
   "source": [
    "## Data Cleanup\n",
    "Based on our inspection, this dataset is uncommonly well formatted and of high quality. But a little cleanup and formatting aids will make some things simpler in future notebooks. \n",
    "\n",
    "One thing that is missing is a list of just station id's and their coordinates. Let's generate that and save it for later. First, let's group by all the unique \"from\" and \"to\" station id values, and take a representative from each group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f735b75-d8fb-4056-bd33-e0aa31f48064",
   "metadata": {},
   "outputs": [],
   "source": [
    "from_ids = df.groupby(\"from_station_id\")\n",
    "to_ids = df.groupby(\"to_station_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5243e66e-8f87-4013-b3a7-964bd489aeab",
   "metadata": {},
   "source": [
    "It's possible (but unlikely) that a particular station is only a sink or source for trips. For good measure, let's make sure the group keys are identical:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8e05aa-484c-4712-98b9-aef0c7cc3b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all(from_ids.size().index.values  == to_ids.size().index.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb37b14-d6b5-4571-afb0-a9e8d3f8b5c2",
   "metadata": {},
   "source": [
    "Each group has items for a single station, which all have the same lat/lon. So let's make a new DataFrame by taking a representative from each group, then rename some columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900554e9-0b49-45a4-aebd-0a99f3460bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations = from_ids.nth(1).to_pandas()\n",
    "stations.index.name = \"station_id\"\n",
    "stations.rename(columns={\"latitude_start\": \"lat\", \"longitude_start\": \"lon\"}, inplace=True)\n",
    "stations = stations.reset_index().filter([\"station_id\", \"lat\", \"lon\"])\n",
    "stations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8991df08-b37c-47f5-837e-2d1ea5e8c1ab",
   "metadata": {},
   "source": [
    "Finally write the results to \"stations.csv\" in our data directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef8ab85-036b-4e9b-8437-d5e0273409fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations.to_csv(DATA_DIR / \"stations.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c417f4-8582-47e3-83c8-341c535f41f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run this cell to show the next section's walkthrough video ##\n",
    "from IPython.display import HTML\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/c0hQAGPdF5U\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498659d6-1945-413f-b244-bace0637e78f",
   "metadata": {},
   "source": [
    "## Summary of the Data\n",
    "Overall this is an interesting and useful dataset. Our preliminary vetting found no issues with quality and already started to hint at areas to investigate:\n",
    "\n",
    "- Weekday vs Weekend trip counts\n",
    "- Bike trips vs weather correlation \n",
    "- Core vs Outward trip concentrations \n",
    "\n",
    "We will follow up with these findings in our next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da34a9f5-33f7-41b8-a0f5-042d71bf34c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233ab367-3a99-4edb-9b31-14840295a842",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64a304a5-6a9a-4667-9488-4702f4670898",
   "metadata": {},
   "source": [
    "# Exploratory Data Visualization\n",
    "***Quickly finding linked patterns in your data***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3837d1-07e0-430d-b94a-6800956d7832",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run this cell to show the next section's walkthrough video ##\n",
    "from IPython.display import HTML\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Xu6R9Tad7H0\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac3e70c-b7c8-4cca-890a-e33536e7d34d",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Taking the previous notebook’s vetted Divvy bike share dataset, we will now use, cuDF, cuxfilter, and cuGraph to quickly create cross-filtered visualizations to explore different perspectives and slices of the data in search of interesting patterns. \n",
    "\n",
    "### cuxfilter and cuGraph\n",
    "- [cuDF](https://docs.rapids.ai/api/cudf/stable/) is a RAPIDS GPU DataFrame library for manipulating data with a pandas-like API.\n",
    "\n",
    "- [cuxfilter](https://docs.rapids.ai/api/cuxfilter/nightly/) is a RAPIDS viz project. Focused around cross-filtering data, its designed to quickly build linked dashboards powered by cuDF compute capabilities. Cuxfilter acts as a connector library rather than a visualization library. It abstracts away all the 'plumbing' required to connect a [curated list of visualizations](https://docs.rapids.ai/api/cuxfilter/nightly/charts/charts.html) to a GPU dataframe. By simply enabling accelerated dashboards inline within a notebook workflow, cuxfilter allows analysts to get to exploring their data faster.\n",
    "\n",
    "- [cuGraph](https://docs.rapids.ai/api/cugraph/stable/) is a RAPIDS GPU accelerated graph analytics library with functionality like NetworkX."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfb001b-4f58-482c-b40c-b4763f9a3a96",
   "metadata": {},
   "source": [
    "## Imports\n",
    "Let's first make sure the necessary imports are present to load, as well as setting the data location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd47fa32-1f20-4d45-8772-0c3dcdecda36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cuxfilter\n",
    "import cudf\n",
    "import cugraph\n",
    "from bokeh.models import NumeralTickFormatter\n",
    "from pyproj import Proj, Transformer\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabfc9b8-2ed3-46b4-91b4-a68779b6bbf8",
   "metadata": {},
   "source": [
    "## Load Data into cuDF\n",
    "As before, load `datda.csv` into the GPU dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a8142c-8039-404f-8de9-e82907258a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"../data\")\n",
    "FILENAME = Path(\"data.csv\")\n",
    "\n",
    "data = cudf.read_csv(DATA_DIR / FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9d8f6b-192a-46c3-9e49-83fed20176aa",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "Before we can visualize the data, we need to do some preprocessing to make it more human readable and usable for cuxfilter.\n",
    "\n",
    "First we need to transform the x/y coordinates from its original [espg4326 projection](https://epsg.io/4326) to the spherical [epsg:3857 projection](https://epsg.io/3857) that works with the maptile underlays used in cuxfilter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8784eb90-be9f-4a6a-bf6a-96afdc5d11dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_coords(df, x='x', y='y'):\n",
    "    transform_4326_to_3857 = Transformer.from_crs('epsg:4326', 'epsg:3857')\n",
    "    df['x'], df['y'] = transform_4326_to_3857.transform(df[x].to_array(), df[y].to_array())\n",
    "    return df\n",
    "# Apply Transformation\n",
    "trips = transform_coords(data, x='latitude_start', y='longitude_start')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2860811-cdfe-4dfd-bcf7-123a60dacd3a",
   "metadata": {},
   "source": [
    "Based on our previous finding about the apparent difference between weekends and weekdays, we will want to extract `day_type` from the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50900cec-d25f-4bd2-9e85-36341dd061f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: days 0-4 are weekedays, days 5-6 are weekends \n",
    "trips['day_type'] = 0\n",
    "trips.loc[trips.query('day>4').index, 'day_type'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2699704e-9960-4b6c-ad01-09de13774696",
   "metadata": {},
   "source": [
    "Choosing the appropriate fidelity of data to show always takes some trial and error. Showing total trips of every day for every year can be noisy, while showing by month is not granular enough. We settled on weeks. That means we will want to get the global week number in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b324391b-f061-44dc-88f5-c4fe234603e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Data always has edge cases, such as the extra week anomalies of 2015 and 2016:\n",
    "# trips.groupby('year').week.max().to_pandas().to_dict() is {2014: 52, 2015: 53, 2016: 53, 2017: 52}\n",
    "# Since 2015 and 2016 have 53 weeks, we add 1 to global week count for their following years - 2016 & 2017\n",
    "# (data.year/2016).astype('int') => returns 1 if year>=2016, else 0\n",
    "year0 = int(trips.year.min()) #2014\n",
    "trips['all_time_week'] = data.week + 52*(data.year - year0) + (data.year/2016).astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78f29ec-d0db-4430-a54e-0ddcb8be0e7c",
   "metadata": {},
   "source": [
    "To make the dashboard values more understandable, we are creating string maps to convert the dataset's numbers to their proper names. Though it may seem trivial, it removes unnecessary ambiguity and helps [reduce cognitive load](https://www.nngroup.com/articles/minimize-cognitive-load/) when our focus needs to be on finding patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5aae36-d9b5-406c-8436-9c9ef6ce5d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a weekday string map\n",
    "days_of_week_map = {\n",
    "    0: 'monday',\n",
    "    1: 'tuesday',\n",
    "    2: 'wednesday',\n",
    "    3: 'thursday',\n",
    "    4: 'friday',\n",
    "    5: 'saturday',\n",
    "    6: 'sunday'\n",
    "}\n",
    "\n",
    "month_map = {\n",
    "    1: 'jan', 2: 'feb', 3: 'mar', 4: 'apr', 5: 'may', 6: 'jun', 7: 'jul', 8: 'aug', 9: 'sep', 10: 'oct', 11: 'nov', 12: 'dec'\n",
    "}\n",
    "day_type_map = {0:'weekday', 1:'weekend', '':'all'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e042053-8e8c-4b22-a5e7-941f9d39a306",
   "metadata": {},
   "source": [
    "Finally, we remove the unused columns and reorganize our dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9ca479-ac35-4baf-91f5-2022d145b9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trips = trips[[\n",
    "    'year', 'month', 'week', 'day', 'hour', 'gender', 'from_station_name',\n",
    "    'from_station_id', 'to_station_id', 'x', 'y', 'from_station_name', 'to_station_name', 'all_time_week', 'day_type'\n",
    "]]\n",
    "trips.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222e5369-9e00-4aac-9a7e-76606bc81c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: save modified trips dataframe to be imported in the final notebok\n",
    "trips.to_parquet(DATA_DIR / 'modified_trips.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec97049-5bcd-4099-851f-7ccf1dc52e18",
   "metadata": {},
   "source": [
    "## cuxfilter Bike Trips Dashboard\n",
    "First lets investigate trip totals by varous time slices by linking the dataframe to cuxfilter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7553f661-8f36-4d93-80b6-a1cb6ca0a16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run this cell to show the next section's walkthrough video ##\n",
    "from IPython.display import HTML\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/QfJYu_8Cfgs\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e417ab57-db29-4246-ae36-44645384fa18",
   "metadata": {},
   "outputs": [],
   "source": [
    "cux_df = cuxfilter.DataFrame.from_dataframe(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542a1542-8908-4092-9788-e9061d6f3bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the charts and widgets to use with the selected columns of data and string maps\n",
    "charts = [\n",
    "    cuxfilter.charts.bar('hour', title='trips per hour'),\n",
    "    cuxfilter.charts.bar('month', x_label_map=month_map),\n",
    "    cuxfilter.charts.bar('day', x_label_map=days_of_week_map),\n",
    "    cuxfilter.charts.multi_select('year'),\n",
    "    cuxfilter.charts.multi_select('day_type', label_map=day_type_map),\n",
    "]\n",
    "\n",
    "# Generate the dashboard and select a layout\n",
    "d = cux_df.dashboard(charts, layout=cuxfilter.layouts.feature_and_double_base, title='Bike Trips Dashboard')\n",
    "\n",
    "# Update the yaxis ticker to an easily readable format\n",
    "for i in charts:\n",
    "    if hasattr(i.chart, 'yaxis'):\n",
    "        i.chart.yaxis.formatter = NumeralTickFormatter(format=\"0,0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6e47cb-711d-471f-8652-b9826269ceb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the dashboard, a green button should appear to open one in a new tab.\n",
    "# Note: use the slider below each chart to cross filter.\n",
    "\n",
    "# IMPORTANT: replace notebook_url with your jupyterhub/binder base url\n",
    "# IMPORTANT: if your notebook environment is in jupyterhub, set service_proxy='jupyterhub', otherwise set to 'none'\n",
    "BASE_URL = 'http://localhost:8888/'\n",
    "d.show(notebook_url=BASE_URL, service_proxy='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e8567f-1ad9-4c87-9797-ba5c7bb3dfa3",
   "metadata": {},
   "source": [
    "### Try It Out\n",
    "Now try using [cuxfilter's user guide](https://docs.rapids.ai/api/cuxfilter/nightly/) and our examples to create a dashboard of the above data using a different layouts, themes, and chart types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe9b958-204a-42e4-ba14-3cf179ce81b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be59f231-d7e7-4ccf-9d46-bf66f97359e5",
   "metadata": {},
   "source": [
    "## cuxfilter Temperature Dashboard\n",
    "Lets continue investigating, this time following up on the increasing trips year over year and decreases in winter months. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6c3361-4bd3-4ba0-a9b0-fa0d16ca610b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run this cell to show the next section's walkthrough video ##\n",
    "from IPython.display import HTML\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/b7Kg9U_M1HM\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a0a141-bdc2-483f-a4df-4ab5ef5cb342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the charts and widgets to use with the selected columns of data and string maps\n",
    "charts = [\n",
    "    cuxfilter.charts.bar('all_time_week', title='rides per week'),\n",
    "    cuxfilter.charts.heatmap(x='all_time_week', y='day', aggregate_col='temperature',\n",
    "                             aggregate_fn='mean', point_size=40, legend_position='right',\n",
    "                             title='mean temperature by day'),\n",
    "    cuxfilter.charts.multi_select('day_type', label_map=day_type_map),\n",
    "]\n",
    "\n",
    "# Generate the dashboard and select a layout\n",
    "d = cux_df.dashboard(charts, layout=cuxfilter.layouts.feature_and_base, title='Temperature Dashboard')\n",
    "\n",
    "# Update the yaxis ticker to an easily readable format\n",
    "for i in charts:\n",
    "    if hasattr(i.chart, 'yaxis'):\n",
    "        i.chart.yaxis.formatter = NumeralTickFormatter(format=\"0,0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aee1347-192b-48b4-8057-919d6b3063d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the dashboard, a green button should appear to open one in a new tab.\n",
    "# Note: pan to match up the top and bottom chart axis\n",
    "\n",
    "# IMPORTANT: replace notebook_url with your jupyterhub/binder base url\n",
    "# IMPORTANT: if your notebook environment is in jupyterhub, set service_proxy='jupyterhub', otherwise set to 'none'\n",
    "BASE_URL = 'http://localhost:8888/'\n",
    "d.show(notebook_url=BASE_URL, service_proxy='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3230bc-06d4-4f03-9054-41ec96c74cca",
   "metadata": {},
   "source": [
    "## Weather Findings\n",
    "The dashboard should look something like this:\n",
    "<img src=\"https://raw.githubusercontent.com/jupytercon/2020-exactlyallan/master//images/cuxfilter_02_dashboard_2.png\" />\n",
    "\n",
    "The weather's effect becomes clear in this dashboard as warmer temperatures seem to strongly match a large increase in ride counts - which intuitively makes sense. But aside developing weather control, there is'nt much that can be done to respond to this finding. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e615a4e8-1291-46d9-9841-f4992054c4f0",
   "metadata": {},
   "source": [
    "## cuxfilter Geospatial Trips Graph\n",
    "Next, lets take a look at the geospatial element of the data and see if we can find interesting patterns. Based on how the trip data is logged, converting it into a graph will make managing it easier.\n",
    "\n",
    "For this we will need [cuGraph](https://docs.rapids.ai/api/cugraph/stable/api.html) to translate the dataset into an edge list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d175980b-8950-4a54-8c1f-a05d153042e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run this cell to show the next section's walkthrough video ##\n",
    "from IPython.display import HTML\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/36yztZl_jfY\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03aadcd4-f35a-4b25-a531-d803d2c94f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = cugraph.Graph() \n",
    "G.from_cudf_edgelist(data, source='from_station_id', destination='to_station_id')\n",
    "edges = G.edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cb7e5b-8ace-482d-8916-8ff1bdbfd5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trips have been converted into edges with source and destination based on station IDs.\n",
    "edges.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9785b2-c863-4ef2-806e-bbb95a18b8dc",
   "metadata": {},
   "source": [
    "Next we load the formatted data into cuxfilter and specify the chart types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b656ff5-1cc1-40a1-90f2-b0f66907a263",
   "metadata": {},
   "outputs": [],
   "source": [
    "cux_df = cuxfilter.DataFrame.load_graph((trips, edges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46af235-18d8-451b-be75-96f25b67229c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying a graph chart type will use Datashader and its required parameters\n",
    "charts = [\n",
    "    cuxfilter.charts.graph(\n",
    "        node_id='from_station_id',\n",
    "        edge_source='src', edge_target='dst',\n",
    "        node_aggregate_fn='count',\n",
    "        node_pixel_shade_type='linear', node_point_size=35, #node size is fixed\n",
    "        edge_render_type='curved', #other option: direct\n",
    "        edge_transparency=0.7, #0.1 - 0.9\n",
    "        tile_provider='CARTODBPOSITRON', \n",
    "        title='Graph for trip source_stations (color by count)'\n",
    "    ),\n",
    "    cuxfilter.charts.multi_select('year'),\n",
    "    cuxfilter.charts.multi_select('day_type', label_map=day_type_map),\n",
    "    cuxfilter.charts.bar('from_station_id'),\n",
    "    cuxfilter.charts.bar('to_station_id'),\n",
    "    cuxfilter.charts.view_dataframe(['from_station_name', 'from_station_id'], drop_duplicates=True)\n",
    "]\n",
    "\n",
    "# Generate the dashboard, select a layout and theme\n",
    "d = cux_df.dashboard(charts, layout=cuxfilter.layouts.feature_and_triple_base, theme=cuxfilter.themes.rapids, title='Geospatial Trips')\n",
    "\n",
    "# Update the yaxis ticker to an easily readable format\n",
    "for i in charts:\n",
    "    if hasattr(i.chart, 'yaxis'):\n",
    "        i.chart.yaxis.formatter = NumeralTickFormatter(format=\"0,0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3849f25a-4c91-403b-bedc-38dfd5e20b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the dashboard, a green button should appear to open one in a new tab.\n",
    "# Note: Graph edges can be turned on/off via the line tool icon\n",
    "# Note: Inspect Neighboring Edges can be turned on/off for box or lasso select\n",
    "# Caution: Selecting areas with Inspect Neighboring Edges on can result in slow performance or OOM errors  \n",
    "# Caution: If the dashboard freezes, simply close the tab and restart this cell\n",
    "# Note: This is rendering 9 MILLION edges\n",
    "\n",
    "# IMPORTANT: replace notebook_url with your jupyterhub/binder base url\n",
    "# IMPORTANT: if your notebook environment is in jupyterhub, set service_proxy='jupyterhub', otherwise set to 'none'\n",
    "BASE_URL = 'http://localhost:8888/'\n",
    "d.show(notebook_url=BASE_URL, service_proxy='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a54f49e-078f-4aba-ac7c-39843872f801",
   "metadata": {},
   "source": [
    "## cuxfilter Network and Geospatial Graph\n",
    "While the above produced many findings, filtering through so many trip edges is not ideal.\n",
    "Next we will try to push the visual analytics further with a clustered network graph along side the geospatial graph using the [ForceAtlas2](https://docs.rapids.ai/api/cugraph/stable/api.html?highlight=force#module-cugraph.layout.force_atlas2) algorithm from cuGraph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df123ab-554d-403e-85bf-6b4c59e03f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run this cell to show the next section's walkthrough video ##\n",
    "from IPython.display import HTML\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/IgLXuW-LRVk\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db001c26-c9d0-4468-95bb-a0e03be2d086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Often a good visualization result only comes from a lot of trial and error\n",
    "# The below parameters produce useful clustering, but try experimenting with them further\n",
    "ITERATIONS=500\n",
    "THETA=10.0\n",
    "OPTIMIZE=True\n",
    "\n",
    "# Using the previously created edge list, we calculate the FA2 layout positions here\n",
    "trips_force_atlas2_layout = cugraph.layout.force_atlas2(G, max_iter=ITERATIONS,\n",
    "                strong_gravity_mode=False,\n",
    "                outbound_attraction_distribution=True,\n",
    "                lin_log_mode=False,\n",
    "                barnes_hut_optimize=OPTIMIZE, barnes_hut_theta=THETA, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6f33b1-4747-4fae-805e-131500f517df",
   "metadata": {},
   "source": [
    "Merge the calculated forceAtlas2 layout with the trip dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20371a21-c742-4d56-8cd0-8674863de1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = trips_force_atlas2_layout.merge(\n",
    "                trips[['from_station_id', 'from_station_name','to_station_id', 'year', 'hour', 'day_type', 'x', 'y']],\n",
    "                left_on='vertex',\n",
    "                right_on='from_station_id',\n",
    "                suffixes=('', '_original')\n",
    ")\n",
    "\n",
    "# Preview\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d35ef42-cff6-4b80-8592-820b77548cff",
   "metadata": {},
   "source": [
    "Next we load the data into cuxfilter and specify the chart types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3a1e23-1377-4bd7-85e2-bbc9582b14ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "cux_df = cuxfilter.DataFrame.load_graph((final_df, edges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e28519-f42d-4332-87dc-6cc13d67cb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Both scatter and graph chart types use Datashader \n",
    "charts= [\n",
    "  cuxfilter.charts.graph(\n",
    "      edge_source='src', edge_target='dst',\n",
    "      edge_color_palette=['gray', 'black'],\n",
    "      ode_pixel_shade_type='linear',\n",
    "      edge_render_type='curved', #other option: direct\n",
    "      edge_transparency=0.7, #0.1 - 0.9\n",
    "      title='ForceAtlas2 Layout Graph'\n",
    "  ),\n",
    "  cuxfilter.charts.scatter(\n",
    "    x='x_original', y='y_original', \n",
    "    tile_provider='CARTODBPOSITRON',\n",
    "    point_size=3,\n",
    "    pixel_shade_type='linear',\n",
    "    pixel_spread='spread',\n",
    "    title='Original Layout'\n",
    "  ),\n",
    "  cuxfilter.charts.multi_select('year'),\n",
    "  cuxfilter.charts.multi_select('day_type', label_map={0:'weekday', 1:'weekend', '':'all'}),\n",
    "  cuxfilter.charts.bar('hour', title='Trips per hour'),\n",
    "  cuxfilter.charts.bar('from_station_id', title='Source station'),\n",
    "  cuxfilter.charts.bar('to_station_id', title='Destination station'),\n",
    "  cuxfilter.charts.view_dataframe(['from_station_id', 'from_station_name'], drop_duplicates=True)\n",
    "] \n",
    "\n",
    "# Generate the dashboard, select a layout and theme\n",
    "d = cux_df.dashboard(charts, layout=cuxfilter.layouts.double_feature_quad_base, theme=cuxfilter.themes.rapids, title=\"Network and Geospatial Graph\")\n",
    "\n",
    "# Update the yaxis ticker to an easily readable format\n",
    "for i in charts:\n",
    "    if hasattr(i.chart, 'yaxis'):\n",
    "        i.chart.yaxis.formatter = NumeralTickFormatter(format=\"0,0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df5acfb-825e-48bc-98c0-f4481e0e0757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the dashboard, a green button should appear to open one in a new tab.\n",
    "# Note: Graph edges can be turned on/off via the line tool icon\n",
    "# Note: Inspect Neighboring Edges can be turned on/off for box or lasso select\n",
    "# Caution: Selecting areas with Inspect Neighboring Edges on can result in slow performance or OOM errors  \n",
    "# Caution: If the dashboard freezes, simply close the tab and restart this cell\n",
    "# Note: This is rendering 9 MILLION edges\n",
    "\n",
    "# IMPORTANT: replace notebook_url with your jupyterhub/binder base url\n",
    "# IMPORTANT: if your notebook environment is in jupyterhub, set service_proxy='jupyterhub', otherwise set to 'none'\n",
    "BASE_URL = 'http://localhost:8888/'\n",
    "d.show(notebook_url=BASE_URL, service_proxy='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cde3a0f-3a5b-45aa-b25c-0c1545feecd5",
   "metadata": {},
   "source": [
    "## Summary of Exploratory Findings\n",
    "Based on the exploratory analytics done above, we've found that there are two distinct groups of behaviors based on time (hour / weekend / weekday) and location. With the next notebook, we will see if we can coax out further information about these groups using more advanced data analytics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440cead3-36eb-4f89-b992-35995bd9fb6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc7f2fa4-7795-402b-ab9c-49e6c145ca06",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0363d07d-06af-4dac-9fbf-fedf5c2f5337",
   "metadata": {},
   "source": [
    "### cuxfilter Troubleshooting\n",
    "As we just released the graph visualization capability in cuxfilter, we are still working on building out features and fixes. \n",
    "\n",
    "If you find something that needs fixing or have feature requests, please submit an [issue on our Github Page](https://github.com/rapidsai/cuxfilter/issues). Better yet, [help contribute](https://github.com/rapidsai/cuxfilter#contributing-developers-guide). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d118a00-fa49-46e8-ae76-0710ed273da7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5021c9-2d4b-45dc-ab53-69f80971502b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "60dd350e-38d1-4d9b-871b-a7601af969a8",
   "metadata": {},
   "source": [
    "# Data Analysis with Visual Analytics\n",
    "\n",
    "***Combining analytics with visualization***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5317f5ff-574e-44e5-bcc8-e550a1bcfad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run this cell to show the next section's walkthrough video ##\n",
    "from IPython.display import HTML\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/tZl0mNmBwrA\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba613d50-7511-4d76-9266-23564b95322a",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In this notebook we will continue to explore the Divvy bikes dataset using, cuDF, cuGraph, and cuSpatial to see how these analysis results can easily feed directly into visualization tools like hvplot and Datashader.\n",
    "\n",
    "### cuDF, cuGraph, cuSpatial, hvplot, and Datashader\n",
    "- [cuDF](https://docs.rapids.ai/api/cudf/stable/), is a GPU DataFrame library for manipulating data with a pandas-like API.\n",
    "\n",
    "- [cuGraph](https://docs.rapids.ai/api/cugraph/stable/) is a RAPIDS library for GPU accelerated graph library with functionality like NetworkX.\n",
    "\n",
    "- [cuSpatial](https://docs.rapids.ai/api/cuspatial/stable/) is a collection of GPU accelerated algorithms for computing geo-spatial measures.\n",
    "\n",
    "- [hvplot](https://hvplot.holoviz.org/) is a high-level plotting API for the PyData ecosystem built on [HoloViews](http://holoviews.org/).\n",
    "\n",
    "- [Datashader](https://datashader.org/) is a library for high fidelity server side data rendering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad21a45-a6da-4c39-8108-394e236dc384",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "In addition to the libraries mentioned above, we will also make use of libraries [cupy](https://docs.cupy.dev/en/stable/), [NumPy](https://numpy.org/), and [Pandas](https://pandas.pydata.org/) directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc21b879-0d36-4dcb-8cec-594456588fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "import cugraph\n",
    "import cupy\n",
    "import cuspatial\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import datashader as ds\n",
    "import datashader.transfer_functions as tf\n",
    "\n",
    "import hvplot.cudf\n",
    "import hvplot.pandas\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652e31a9-b522-4287-9fd2-c6916ad0f01d",
   "metadata": {},
   "source": [
    "## Loading Data into cuDF\n",
    "\n",
    "First let's load the data. In addition to the main Divvy `data.csv` file, we will also load the small `stations.csv` file that we prepared in the first notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a70c248-17a7-466f-ac98-cf6d18721e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run this cell to show the next section's walkthrough video ##\n",
    "from IPython.display import HTML\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/ZeiLc_DbKEk\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c89e2ac-ab4d-43b8-8359-4515a1c3f3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7661b062-7df1-4d8d-85db-c4bdee439924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: remember to reparse into datetimes\n",
    "df = cudf.read_csv(DATA_DIR / \"data.csv\", parse_dates=('starttime', 'stoptime'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf98f5c3-c24e-4f7e-9f62-891e7ad1aba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations = pd.read_csv(DATA_DIR / \"stations.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25e77ae-1d61-4d35-af75-f915a979b541",
   "metadata": {},
   "source": [
    "We will want to continue our investigation into weekday vs weekend patterns, so let's first add a column for that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d924b9d0-be8a-40e8-b748-068e7a6bbb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"weekday\"] = df['starttime'].dt.weekday"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54560d4e-9d67-4c46-a1a5-0c760fc1a37e",
   "metadata": {},
   "source": [
    "## Trying Analysis with CuSpatial \n",
    "\n",
    "Let's take a look at some spatial measures and see if there are any interesting features.\n",
    "\n",
    "We might start with the first station, and see what the max trip length from it is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4571b1-5c03-4ead-a2f4-8f9be4c4f5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "r0 = df.iloc[0]\n",
    "station_id, origin_lon, origin_lat = r0[\"from_station_id\"], r0[\"longitude_start\"], r0[\"latitude_start\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e012d29c-1da4-4c87-8486-19fedcab90bd",
   "metadata": {},
   "source": [
    "The cuSpatial function `lonlat_to_cartesian` will let us quickly compute the x/y distances for every ending trip location (in Kilometers):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4911c72-a108-4423-8efa-726c7d1e0c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = df[df[\"from_station_id\"]==station_id[0]]\n",
    "dist = cuspatial.lonlat_to_cartesian(origin_lon[0], origin_lat[0], sub_df[\"longitude_end\"], sub_df[\"latitude_end\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0777e9-6973-4af2-a22e-eaab7b35c94c",
   "metadata": {},
   "source": [
    "CuPy functions can compute derived values on these GPU dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e26a12-838b-45d5-9b5f-bfe3675335ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# good o' pythagorean theorem\n",
    "cupy.sqrt(cupy.max(dist.x**2 + dist.y**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f066a95-b889-4f42-a8da-65e840827c6c",
   "metadata": {},
   "source": [
    "What if we want to compute all trip distances? We can compute the distances using every station as a starting point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cab9f93-38f5-4d96-9ab8-7361ec359b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trip_dists(df):\n",
    "    results = []\n",
    "\n",
    "    for idx, row in stations.iterrows():\n",
    "        station_id, origin_lon, origin_lat = int(row[\"station_id\"]), row[\"lon\"], row[\"lat\"]\n",
    "        sub_df = df[df[\"from_station_id\"]==station_id]\n",
    "        res = cuspatial.lonlat_to_cartesian(origin_lon, origin_lat, sub_df[\"longitude_end\"], sub_df[\"latitude_end\"])\n",
    "        res[\"dist\"] = cupy.sqrt(res.x**2 + res.y**2)\n",
    "        results.append(res)\n",
    "        \n",
    "    return cudf.concat(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0490ee7e-b92f-4d28-9511-11f64607a35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_from_dists = trip_dists(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6393c288-7c09-4c2d-bd46-e974f805d437",
   "metadata": {},
   "source": [
    "## hvplot of Trip Distances\n",
    "Now that we have all the distances in a dataframe, we can use hvplot to create a plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277b2f11-dcb0-4167-84ab-f7822c0ce247",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# bin size is chosen after some trial and error\n",
    "all_from_dists.hvplot.hist(y=\"dist\", normed=True, bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5206b1-28c2-46f4-8538-0fdb1d51a387",
   "metadata": {},
   "source": [
    "Clearly most trips are fairly short -usually under 2Km. This makes sense when we remember most trip durations are also less than 15min. \n",
    "\n",
    "It might also be interesting follow up and break the distribution of trips down weekday vs weekend:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2dcdf1f-6a20-4eb7-8530-7ed4a328d69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "weekend_trips = df[df[\"weekday\"].isin([5, 6])] # weekend days = 5, 6 \n",
    "weekday_trips = df[df[\"weekday\"].isin(list(range(5)))]  # weekday days = 0-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbcab01-cb73-4838-8245-2974dbcaba71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate distances from the previous function\n",
    "weekend_dists = trip_dists(weekend_trips)\n",
    "weekday_dists = trip_dists(weekday_trips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68052980-8078-485f-9691-8c071d86f0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_combined_dists =  cudf.concat([weekday_dists, weekend_dists])\n",
    "all_combined_dists.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4387c67-76c5-4bfd-9601-4653cf864548",
   "metadata": {},
   "source": [
    "## hvPlot of Weekend vs Weekday Trip Distance\n",
    "Plotting these two distributions together we can see the weekday (orange) trips peak more at shorter distances and the weekend distributions (blue) has more, longer trips:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9061a538-fff8-4c78-a439-5660e50f3f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run this cell to show the next section's walkthrough video ##\n",
    "from IPython.display import HTML\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/9qnnVF91Xfc\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d230c4-a2ca-4b41-948c-8ec2019db2ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weekend_hist = weekend_dists.hvplot.hist(y=\"dist\", alpha=0.5, bin_range=(0, 10), normed=True, color=\"blue\")\n",
    "weekday_hist = weekday_dists.hvplot.hist(y=\"dist\", alpha=0.5, bin_range=(0, 10), normed=True, color=\"orange\")\n",
    "weekend_hist * weekday_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8de167c-e59c-4c54-9401-185c512f59f8",
   "metadata": {},
   "source": [
    "While interesting to note, there doesn't appear to be any major revelations using a distance analysis approach. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5022cb7-200c-433c-b753-b22bad5112e6",
   "metadata": {},
   "source": [
    "## Trying Analysis with cuDF\n",
    "\n",
    "Let's use CuDF direclty to group and aggregate our data to look for anyting intersting about the flow of trips in and out stations. \n",
    "\n",
    "We want to look at the daily net flow of trips at each station, i.e. how many more (or less) trips *started* at a station vs *ended* at a station in a given day.\n",
    "\n",
    "In order to group by day, we first take the \"floor\" of each timestamp divided by one day:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10c9b6f-7d89-4b54-ae5b-5697a64df0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_day = np.datetime64(1, 'D').astype('datetime64[ns]').astype('int64') \n",
    "\n",
    "# out\n",
    "df['from_day'] = df['starttime'].astype('int64') // one_day\n",
    "\n",
    "# in\n",
    "df['to_day'] = df['stoptime'].astype('int64') // one_day"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffa31d6-32ca-4888-afe7-afc58762b3ae",
   "metadata": {},
   "source": [
    "Now we can group by the station id and hour for both the departing and arriving cases. We name the columns from the size DataFrame `out` and `in` respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1da306-099f-4906-87df-88051d082d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out = df.groupby(by=[\"from_station_id\", \"from_day\"]).size().to_frame('out').reset_index()\n",
    "df_in = df.groupby(by=[\"to_station_id\", \"to_day\"]).size().to_frame('in').reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac48c12-959d-42ea-8c15-206648a4d450",
   "metadata": {},
   "source": [
    "Let's rename the columns to be the same in both DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c75bbb6-ded4-4c5c-bd76-4b4fcd34adc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out.rename(columns={\"from_station_id\": \"station_id\", \"from_day\": \"day\"}, inplace=True)\n",
    "df_in.rename(columns={\"to_station_id\": \"station_id\", \"to_day\": \"day\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b288dc3f-aecf-4443-a7b9-60a556d6dadf",
   "metadata": {},
   "source": [
    "And reset the index to be the (station id, hour) pair:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df9c653-2f84-405d-a900-95484257693a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out = df_out.set_index([\"station_id\", \"day\"])\n",
    "df_in = df_in.set_index([\"station_id\", \"day\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c6b2f8-614f-4064-80aa-5a1e0264ae5b",
   "metadata": {},
   "source": [
    "Now we can join these two DataFrames to compute an `flow = out - in` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5eb0e99-8ff8-4fe1-8708-3925e8f86dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = df_in.join(df_out, how=\"outer\").fillna(0).reset_index()\n",
    "full_df[\"flow\"] = full_df[\"out\"] - full_df[\"in\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28632080-ab4a-492f-8387-1cb6d5cc71a7",
   "metadata": {},
   "source": [
    "Let's also convert our \"day\" values back to proper timestamps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967701fe-0bf5-46ec-9053-ea519ce3ea2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df[\"time\"] = (full_df[\"day\"] * one_day).astype('datetime64[ns]')\n",
    "full_df = full_df[[\"station_id\", \"time\", \"flow\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a59775-58e1-4823-9045-560a5eec1475",
   "metadata": {},
   "source": [
    "Now we can take a glimpse at the resulting DataFrame which has the net `out-in` trip flow by station per day:\n",
    "- A `+` positive number means there was an excess of trips *starting out* of the station that day.\n",
    "- A `-` negative number means an excess of trips *ending in* the station that day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afaa1155-e15b-4476-a247-707cc52d553c",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9888c1e4-0dfe-4436-8a58-db2a3908e7aa",
   "metadata": {},
   "source": [
    "We might like to look at the maximal behaviour. What is a high number of excess arrivals or departures at a station? Let's pull out individual timeseries for each station id, and look a the max/min for each station:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfae515-f3c3-406f-9b9d-542e0ba6ff49",
   "metadata": {},
   "outputs": [],
   "source": [
    "flows = []\n",
    "for i in stations.station_id:\n",
    "    subdf = full_df[full_df.station_id==i].set_index(\"time\")\n",
    "    flows.append((i, subdf.flow.max(), subdf.flow.min()))\n",
    "flows = pd.DataFrame(flows, columns=[\"station_id\", \"max_out\", \"max_in\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e348dd91-f3d1-4ae2-a997-7be14e47a0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "flows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d041a4bf-365b-4db2-a42c-d149cdaa0797",
   "metadata": {},
   "source": [
    "With this information, we can see what stations had the largest ever excess departures (station 192) or arrivals (station 77):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6bb61a-685f-4af0-996f-0be39c6d522b",
   "metadata": {},
   "outputs": [],
   "source": [
    "flows.iloc[flows.max_out.argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4232d9d-04aa-40ab-ab52-a0bf6ab55f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "flows.iloc[flows.max_in.argmin()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b158d60-5172-4159-b397-00cfc466469c",
   "metadata": {},
   "source": [
    "Knowing about excess arrivals vs departures is probably important for Divvy to be able to manually re-allocate bikes. We could ask what fraction of stations ever have a max of more than 30 excess trips:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37480ecc-e2e7-4fd3-ae55-7ecaa7590718",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(flows[flows.max_out > 30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0e6916-2ce0-48c1-ac5c-d0944139efb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(flows[flows.max_in < -30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543f06ea-1fa9-48ee-94ed-1d7868cab40f",
   "metadata": {},
   "source": [
    "While looking at individual stations or max/mins is useful to get preliminary ideas of patterns, it would be better to see it all at once. First we need to prepare a new Dataframe that has all the series as columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccde60b-b33f-4eec-9944-74ff7cbc54a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "series = []\n",
    "\n",
    "for i in stations.station_id:\n",
    "    s = full_df[full_df.station_id==i][[\"time\", \"flow\"]]\n",
    "    s.rename(columns={\"flow\": f\"s{i}\"}, inplace=True)\n",
    "    s = s.set_index(\"time\")\n",
    "    series.append(s)\n",
    "    \n",
    "df_wide = cudf.concat(series, axis=1).fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59cb23e-8b1c-48d7-a01f-d9d11176358f",
   "metadata": {},
   "source": [
    "The resulting Dataframe has a daily time series for every column, one for each station:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3136b4-c1f0-44aa-9912-94ba1875326b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8e7181-e873-4b16-9ac0-907e0539f003",
   "metadata": {},
   "source": [
    "## hvplot of Select Station Flows\n",
    "It's simple to pull out individual stations for comparison using `hvplot`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ad63e3-d98b-4cba-8bb6-3703c64a3e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wide.hvplot(y=[\"s77\",\"s81\",\"s192\",\"s195\",\"s268\",\"s287\"], alpha=0.4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d94da38-d753-4e0a-abec-2f5c6ebf9346",
   "metadata": {},
   "source": [
    "The above plot shows some of the more interesting station patterns - which roughly match the overall seasonal flows. Station 195 appears perpetually over taxed, while something nearby station 77 seems to draw in a lot of bikes. Yet its hard to gleen a pattern without the connection between station's flows. \n",
    "\n",
    "Bonus points for anyone who knows what anomaly happened on 6/24/2014 (seriously, we're curious). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32824ade-3334-488b-8a42-4faa9c26b228",
   "metadata": {},
   "source": [
    "### Try It Now\n",
    "See if you can plot all the stations in an hvplot (it is possible but takes a while to render): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c78fe17-817f-4c58-a828-33e404d677ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b478cb-1ca3-49bc-aa0c-001849e98615",
   "metadata": {},
   "source": [
    "Lastly, lets take a look at the data with Datashader. First we make a funtion `series_shade` that can take a wide dataframe of timeseries like we have made above, and render *all* of the series at once using Datashader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c611fb-34f0-4809-9e63-d0ea7aacd748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Details here https://datashader.org/user_guide/index.html\n",
    "def series_shade(df):\n",
    "    cols = list(df.columns)\n",
    "    \n",
    "    itime = cudf.to_datetime(df.index).astype('int64')\n",
    "    x_range = (itime[0], itime[-1])\n",
    "    \n",
    "    y_range = (df.min().min(), df.max().max())\n",
    "    \n",
    "    temp = cudf.DataFrame(df)\n",
    "    temp[\"itime\"] = itime\n",
    "    \n",
    "    # the width is 4x365, leaving one pixel width per day\n",
    "    cvs = ds.Canvas(plot_height=400, plot_width=1460)\n",
    "    agg = cvs.line(temp, x=\"itime\", y=cols, agg=ds.count(), axis=1)\n",
    "    \n",
    "    print(f\"y range: ({y_range[0]}, {y_range[1]})\")\n",
    "    return tf.shade(agg, how='linear', cmap=[\"purple\",\"red\",\"white\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d4e1c4-ea44-4edd-a6ae-b06900923b42",
   "metadata": {},
   "source": [
    "## Datashader Line Plot of Total Daily Flows\n",
    "Now let's pass in-out daily net excess data to get a rough datashder plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8842169-4635-4044-9bf5-9c2eb98ea8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "series_shade(df_wide)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8900681-4bfc-4f9e-bab6-5694c494b10e",
   "metadata": {},
   "source": [
    "It's not completely clear what we can see here, but it points to some ideas for future exploration. If you squint it does seem that there is an unbalanced flow out of stations vs into stations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32977aa-0fb9-4ff7-8685-eefb00170802",
   "metadata": {},
   "source": [
    "## Datashader Line Plot of Cumulative Daily Flows\n",
    "As a last experiment, let's make the same plot, but with *cumulative* excess trips:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee75fa8-4b7f-47ab-a42f-884ce1d8a7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cumulative = df_wide.cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bc0d33-166f-4a2a-a00e-b74d5f110189",
   "metadata": {},
   "outputs": [],
   "source": [
    "series_shade(df_cumulative)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7e8b98-cfce-48c8-8fe2-1c9d83d29b6b",
   "metadata": {},
   "source": [
    "This view emphasizes the unbalanced flow and is a bit more interesting. It illustrates the notion that Divvy must be engaging in a lot of continual re-allocation of its bikes to offset these excess trips at individual stations.\n",
    "\n",
    "If we knew the marginal costs compared to ridership income, it could prove an interesting data point on when network expansion would become prohibitive. However, without that we need to look elsewhere for analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352d6a21-1ac5-48b5-ac72-4b33ca5d5317",
   "metadata": {},
   "source": [
    "### Try It Now\n",
    "Datashader plots can be wrapped in hvplots, much like bokeh plots. Try wrapping the above examples in order to make them more interactive by using [Datashader's usage guide](https://datashader.org/user_guide/Timeseries.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6eb1f5-3d45-46ee-9c36-dcb2eaee70a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c700331-d697-4ffa-860c-e8a1eddc0a39",
   "metadata": {},
   "source": [
    "## Trying Analysis with cuGraph PageRank\n",
    "\n",
    "In our previous notebook we were able to find some interesting patterns by converting our dataframe into a graph. Here, we will try the `cugraph.pagerank` algorithm to see if it helps succinctly illustrate patterns for the \"most popular\" stations.\n",
    "\n",
    "First, let's see what it looks like to compute PageRank for a single hour of the day, e.g. 5PM, by subsetting the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521ae453-0de5-4546-b83b-e55dffdce6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run this cell to show the next section's walkthrough video ##\n",
    "from IPython.display import HTML\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/bJushO0ebrg\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff0a425-3518-470b-8a6f-7652dfde5907",
   "metadata": {},
   "outputs": [],
   "source": [
    "d17 = df[df[\"hour\"]==17]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10c8675-2845-4d38-b141-3931dd6f1e18",
   "metadata": {},
   "source": [
    "Then groupby (from_station_id, to_station_id) and take the group size to get all the unique individual routes between stations that hour, and also the number of trips that took each of those routes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf0e242-1b7b-46fb-88fe-0de943d948e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "g17 = df.groupby(by=[\"from_station_id\", \"to_station_id\"])\n",
    "routes17 = g17.size().reset_index()\n",
    "routes17.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed85222b-885c-4ca6-8986-44e2461f1f57",
   "metadata": {},
   "source": [
    "Now we can create a `cugraph.Graph` with the from and to station IDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6d172f-4e9c-40b5-ab94-dd050e2a10a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = cugraph.Graph()\n",
    "G.from_cudf_edgelist(d17, source='from_station_id', destination='to_station_id')\n",
    "d17_page = cugraph.pagerank(G)\n",
    "d17_page.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3d298c-55ad-4e92-ac02-2c33189585cd",
   "metadata": {},
   "source": [
    "PageRank values are relative, and as such do not matter as much as the ranking it produces for the network of trips. Let's see which stations rank as most important at 5PM (on any day):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b7e522-5c15-4151-804d-c6686d1a392f",
   "metadata": {},
   "outputs": [],
   "source": [
    "d17_top = d17_page.nlargest(20, \"pagerank\").to_pandas()\n",
    "d17_top.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d82214b-aad4-4d44-8209-06e6e923a87e",
   "metadata": {},
   "source": [
    "## hvplot of 5pm Top PageRank Locations\n",
    "Plotting these stations we can see that at 5PM the most important stations are nearly all downtown, matching our previous notebook findings about a focused downtown core of total trips:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4b0555-df49-4a3e-b59a-0ecc2ac2cdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "d17_page_locs = stations[stations.station_id.isin(d17_top.vertex)]\n",
    "d17_page_locs.hvplot.points(x='lon', y='lat', alpha=0.7, size=300, geo=True, tiles=\"OSM\").opts(width=600, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f10353-a0a0-40d0-8807-538b251b66a2",
   "metadata": {},
   "source": [
    "Now that we know applying PageRank seems to produce useful results, let's look at how stations rank by weekdays vs weekends. To get proper rankings, we need to compute it for every individual day of the week:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8d9658-36a2-4efe-b3ce-69d51b688309",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for w in range(7):\n",
    "    dfw = df[df[\"weekday\"]==w]\n",
    "    G = cugraph.Graph()\n",
    "    G.from_cudf_edgelist(dfw, source='from_station_id', destination='to_station_id')\n",
    "    df_page = cugraph.pagerank(G).nlargest(20, \"pagerank\")\n",
    "    results[w] = set(df_page.to_pandas()[\"vertex\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e940a0-405f-4841-ba1f-b9fb4a2d7f18",
   "metadata": {},
   "source": [
    "Let's find out what stations were continually highest ranked among all weekdays and weekend days:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d87b79-4258-45e2-87f6-450a33baca00",
   "metadata": {},
   "outputs": [],
   "source": [
    "weekday = set.intersection(*[results[i] for i in range(5)]) # days 0-4 are weekdays\n",
    "weekend = set.intersection(results[5], results[6])  # days 5-6 are the weekend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490d63c8-98d8-4b1c-b02d-18a1caf4fc40",
   "metadata": {},
   "source": [
    "Listing the stations that are ranked important on weekdays and ranked important on weekends, we find that there is little overlap:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0019ee-fb93-402e-a4d7-63b5210b9f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "weekend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33519937-1910-4f0f-a26d-f389461696fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "weekday"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6381a9-c455-4f36-8d2c-447f132994ba",
   "metadata": {},
   "source": [
    "Finally, we can plot these quickly using hvplot again. Let's add a column to denote weekday / weekend so that we can group by that type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88fd519-a5e8-4529-b81c-9c381b9e2f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = stations[stations.station_id.isin(weekend)]\n",
    "r1 = r1.assign(type=\"Weekend\")\n",
    "\n",
    "r2 = stations[stations.station_id.isin(weekday)]\n",
    "r2 = r2.assign(type=\"Weekday\")\n",
    "\n",
    "result = pd.concat([r1, r2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e638d7-390c-41b9-8b61-0168e5419481",
   "metadata": {},
   "source": [
    "## hvplot of Weekend / Weekday Top PageRank Locations\n",
    "Looking at the plot, nearly all the important weekday stations are downtown, and on the weekend the important stations are further out in popular districts around downtown:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7d402f-d7bb-4cdb-b7d6-7cafbafb38ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.hvplot.points(x='lon', y='lat', by='type', \n",
    "                     alpha=0.7, size=300, geo=True, tiles=\"OSM\").opts(width=700, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624adf39-703b-4771-9fb4-ff2df3ab8067",
   "metadata": {},
   "source": [
    "The above map of top PageRanked stations for weekend / weekdays matches very well with the ForceAtlas2 clustered graph and time series cross-filtered visualizations of the previous notebook, but in a much more concise and presentable manner. This is the positive result we were hoping to find with our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b07348-29de-49f4-ab48-0d51a6ecbeb1",
   "metadata": {},
   "source": [
    "## Summary of Analytics Findings \n",
    "When running analytics, its critical to have a solid understanding of the underlying data in order to make correct decisions. We tried several analytical approaches to see if we could glean some meaningful patterns. As is often the case, some worked better than others - but because we did extensive exploratory visualization we now have confidence that the weekend / weekday binned PageRank approach will produce accurate results when used for visualizations in our next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1180dda9-d4b1-4c30-869c-0e6210c8e96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run this cell to show this section's walkthrough video ##\n",
    "from IPython.display import HTML\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/8lfO8gOOTXI\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ff712f-5b84-45a7-b9db-71aa28bae86c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5c86d9-98fd-4121-854e-11a6dddec0f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e38f5880-8d36-4edd-aff8-14e622108d6c",
   "metadata": {},
   "source": [
    "# Explanatory Data Visualization\n",
    "***Interactive presentation dashboards***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e10173-ac14-48b9-8e63-0c917e01f371",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run this cell to show the next section's walkthrough video ##\n",
    "from IPython.display import HTML\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/GJnUGqYj7D0\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d589d9-6c8d-4021-bca5-aa6fecddcc9a",
   "metadata": {},
   "source": [
    "## Overview\n",
    "This final notebook is geared towards taking the previous findings and preparing them for presentation through an interactive Plotly Dash visualization application powered by cuDF and cuGraph's PageRank. \n",
    "\n",
    "### cuDF, cuGraph, cuxfilter, and Plotly Dash\n",
    "- [cuDF](https://docs.rapids.ai/api/cudf/stable/) is a RAPIDS GPU DataFrame library for manipulating data with a pandas-like API.\n",
    "\n",
    "- [cuGraph](https://docs.rapids.ai/api/cugraph/stable/) is a RAPIDS library for GPU accelerated graph analytics with functionality like NetworkX.\n",
    "\n",
    "- [cuxfilter](https://docs.rapids.ai/api/cuxfilter/nightly/) is a RAPIDS visualization library for cross-filtering data, designed to quickly build linked dashboards powered by cuDF compute capabilities. \n",
    "\n",
    "- [Plotly Dash](https://plotly.com/dash/) is a framework for specifying production ready visualization applications all in Python. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d996409b-8f11-4234-94e3-50ebbf83a27f",
   "metadata": {},
   "source": [
    "## Dashboard Concepts and Audiences\n",
    "We've taken a bike share dataset, explored it, run analytics on it, and now have confidence in our ability to highlight the most important stations for two key usage patterns. The next step is communicating our findings - something at which visualization excels.\n",
    "\n",
    "However, a viz needs to be appropriate for the data it is showing, the audience it is intended to be shown to, and the medium or context it will be shown.\n",
    "\n",
    "For instance, is the presentation to highly technical colleagues already familiar with your work, as you drive the presentation from your personal machine? Or to executives at a board room? Or completely asynchronous through a web site with a wide range of audience expertise levels?\n",
    "\n",
    "Thinking about these ahead of time and preparing for them will lead to more successfully communicating your findings.\n",
    "\n",
    "As you think of this, it helps to explore previous works, such as the [Plotly Dash Gallery](https://dash-gallery.plotly.host/Portal/), for ideas and to see best practices (or worst practices, I'm looking at you 3D pie chart...).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49099159-41ad-4bce-9fdb-cf48512d018a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run this cell to show the next section's walkthrough video ##\n",
    "from IPython.display import HTML\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/JyZs4ApG7yI\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2b544e-be7f-4db9-a23c-13baa4959d11",
   "metadata": {},
   "source": [
    "### A Note About Sketching and Design Iterations\n",
    "<img src=\"https://raw.githubusercontent.com/jupytercon/2020-exactlyallan/master/images/dashboard-sketch-ideas.jpg\" />\n",
    "<img src=\"https://raw.githubusercontent.com/jupytercon/2020-exactlyallan/master/images/plotly_dashboard_sketch.jpg\" />\n",
    "\n",
    "A well designed visualization owes as much to iteration as it does to skill (though experience helps). The more iterations, generally the better the viz. However, the mental overhead of our technical tools often get in the way of our thought process.\n",
    "\n",
    "By the time it takes to create a new notebook cell, look up the syntax for your favorite viz library, and load data - you could have quickly generated several iterations of ideas through sketches. \n",
    "\n",
    "As shown from our sketches above, they don't have to be high fidelity or even good - just enough to try out ideas quickly and communicate to colleagues. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ce004d-f44b-4806-ad5f-e4bdd66321bd",
   "metadata": {},
   "source": [
    "### Try It Now\n",
    "Pull out a piece of paper and do a few thumbnail sized sketches of variations on this dashboard - spending no more than 5-10 minutes. The messier the better. \n",
    "\n",
    "Thumbnail sketches are for thinking and personal consumption. Larger ones come later to help communicate ideas to colleagues. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e88a181-d00f-4ccd-a5c7-a576f2209588",
   "metadata": {},
   "source": [
    "## Imports\n",
    "Now that we have sketched out our idea, lets prototype them. As usual, make sure the necessary imports are present to load, as well as setting the data location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66af752-f17f-434e-9968-dcb17d0a0163",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run this cell to show the next section's walkthrough video ##\n",
    "from IPython.display import HTML\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/R5AbKqo2Uvk\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740d42f6-3115-423c-93b7-d077639c91fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from jupyter_dash import JupyterDash\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "from dash.dependencies import Input, Output\n",
    "import pandas as pd\n",
    "import cugraph\n",
    "import cuxfilter\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = Path(\"../data\")\n",
    "FILENAME = Path(\"modified_trips.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338bd15f-c5f6-403a-9230-aaec6394d5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trips = cudf.read_parquet(DATA_DIR / FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47033e41-010d-4954-9547-8b81d2dc2f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "trips['time_of_day'] = 0 # day\n",
    "trips.loc[trips.query('hour>19 or hour<8').index, 'time_of_day'] = 1 # night"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57dfd8f-8380-41e2-9e5f-46205c3b86ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a day_type string map\n",
    "day_type_map = {0:'weekday', 1:'weekend', '':'all'}\n",
    "time_of_day_map = {0:'day(8am-8pm)', 1:'night(8pm-8am)', '':'all'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97e885c-526d-4ca0-81a3-da19feaf05d4",
   "metadata": {},
   "source": [
    "### Dashboard Mockup with cuxfilter\n",
    "With our sketch idea of what we want the final explanatory visualization to look like, and what data it will show, lets try and create an interactive mockup to test our concept.\n",
    "\n",
    "As usual, we will load the data into cuDF and spec out the cuxfilter chart:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c206d0-0108-4537-938f-58bfd913493a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cux_df = cuxfilter.DataFrame.from_dataframe(trips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90ffe87-c311-44ae-9c76-6077b3010f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying a scatter plot chart will use Datashader and its required parameters\n",
    "charts = [\n",
    "    cuxfilter.charts.scatter(x='x', y='y', tile_provider='CARTODBPOSITRON',\n",
    "                           point_size=3, pixel_shade_type='linear', pixel_spread='spread',\n",
    "                          title='All Trips'),\n",
    "    cuxfilter.charts.bar('all_time_week', title='Rides per week'),\n",
    "    cuxfilter.charts.multi_select('day_type', label_map=day_type_map),\n",
    "    cuxfilter.charts.multi_select('hour'),\n",
    "]\n",
    "\n",
    "# Generate the dashboard, select a layout and theme\n",
    "d = cux_df.dashboard(charts, layout=cuxfilter.layouts.feature_and_base, theme=cuxfilter.themes.rapids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d82d49-9d4e-4d07-a900-ead558edf26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: replace notebook_url with your jupyterhub/binder base url\n",
    "# IMPORTANT: if your notebook environment is in jupyterhub, set service_proxy='jupyterhub', otherwise set to 'none'\n",
    "BASE_URL = 'http://localhost:8888/'\n",
    "d.show(notebook_url=BASE_URL, service_proxy='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbae4da-f6f5-4d3f-9365-05d5b5b3bf13",
   "metadata": {},
   "source": [
    "## Mockup Results\n",
    "The cuxfilter mockup should look something like this:\n",
    "<img src=\"https://raw.githubusercontent.com/jupytercon/2020-exactlyallan/master/images/notebook_04_dashboard_1.png\" />\n",
    "\n",
    "Overall the design seems to work, with the obvious caveat that we have yet to see PageRank in action. Nevertheless, because of how quick it is to build an interactive dashboard with cuxfilter, it can work well as a mock up tool.\n",
    "\n",
    "If your design calls for chart types or features which are difficult to fully test (like arbitrary function calls), mocking up still makes sense for even component elements or interactions, supplemented with tools like hvplot or Datashader.\n",
    "\n",
    "Mock ups are particularly important if you haven't connected real or a full set of your data to a visualization yet, since building an explanatory / production level application takes substantial effort (even with a simple API). Therefore, skipping lower fidelity interactive mockups will almost certainly end up wasting time on rework. With data visualization there are always surprises from unuseable results, slow performance, or the limitations of various chart interactions.\n",
    "\n",
    "As mentioned above with sketching, increasing the number of design iterations your visualization goes through improves its quality, and interactive mockups are a useful tool for that purpose.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e60254b-a3fd-4077-a747-c6016a56d53a",
   "metadata": {},
   "source": [
    "## Production Ready <br> Plotly Dash Visualization Application <br> with Real-time Page Rank Compute\n",
    "Now that we are confident that our chart types and interactions are appropriate, lets build the dashboard with help from the [Plotly Dash Documentation](https://dash.plotly.com/).\n",
    "\n",
    "First lets load the data into a cuDF and prepare it for the vis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8df47b-3aa7-4530-b675-68580bfb518e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run this cell to show the next section's walkthrough video ##\n",
    "from IPython.display import HTML\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/nOrVlzvT5Eg\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2c39c7-fcb4-4c7d-b823-c4596eb969af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the stations data from first notebook\n",
    "stations = cudf.read_csv(DATA_DIR / \"stations.csv\")\n",
    "\n",
    "# Get station names\n",
    "station_names = trips[['from_station_id', 'from_station_name']].drop_duplicates()\n",
    "station_names.columns = ['station_id', 'station_name']\n",
    "\n",
    "# Get total trips per station\n",
    "total_trips = (trips.groupby('from_station_id').size() + trips.groupby('to_station_id').size()).reset_index()\n",
    "total_trips.columns = ['station_id', 'total_trips']\n",
    "\n",
    "# Add total trips to dataframe\n",
    "stations = stations.merge(total_trips, on='station_id')\n",
    "stations = stations.merge(station_names, on='station_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86eea5b2-1808-4931-9508-666a155ff2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c5e8be-5716-4d37-842f-b40f894e65f5",
   "metadata": {},
   "source": [
    "## Define Application Layout and Style\n",
    "Plotly Dash apps use standard web elements to define layouts and styles through `html`, `styles`, `class` and `css`. You can learn more on the [Dash layouts documentation](https://dash.plotly.com/layout). For our example we are using a locally hosted `css` file in the default folder: `/assets/dash-style.css` and inline `sytles.` \n",
    "\n",
    "Here we define our app and the layout to have a title `h1` tag, a `div` side bar for total trips and two drop down menus, and another `div` to contain the map chart and bar chart below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f613ddf-900b-4503-ba82-0b962d32685a",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = JupyterDash(__name__)\n",
    "\n",
    "app.layout = html.Div([\n",
    "    html.Div([\n",
    "        html.H3([\"Divvy Bikeshare Chicago\"]),\n",
    "        html.H5([\"Total Selected Trips:\"]),\n",
    "        dcc.Loading(\n",
    "            dcc.Graph(id = 'number', figure = go.Figure(go.Indicator(mode = \"number\", value = trips.shape[0])),\n",
    "            style = {\n",
    "            'height': '250px'\n",
    "            }),\n",
    "            color = '#b0bec5'\n",
    "        ),\n",
    "        html.H5([\"Day of Week:\"]),\n",
    "        dcc.Dropdown(id = 'day', clearable = False, value = '',\n",
    "            options = [{'label': day_type_map[c],'value': c} for c in day_type_map]\n",
    "        ),\n",
    "        html.H5([\"Time of Day:\"]),\n",
    "        dcc.Dropdown(id = 'time', clearable = False, value = '',\n",
    "            options = [{'label': time_of_day_map[c], 'value': c} for c in time_of_day_map]\n",
    "        )],\n",
    "        style = {\n",
    "            'z-index' : '99',\n",
    "            'position': 'absolute',\n",
    "            'width': '15%',\n",
    "            'height': 'calc(100% - 2em)',\n",
    "            'padding': '1em 2em',\n",
    "            'background-color': '#aabacc',\n",
    "            'color': 'rgb(70, 105, 130)',\n",
    "            'box-shadow': '5px 0px 3px 0px rgba(0,0,0,0.1)'\n",
    "        }\n",
    "    ),\n",
    "    html.Div([\n",
    "        html.Div([\n",
    "            html.H5([\"Station Importance PageRank(Color) by Trips(Size)\"]),\n",
    "            dcc.Graph(id = 'pagerank_plot',\n",
    "                config = {'responsive': True, 'modeBarButtonsToRemove': ['select2d', 'lasso2d']}\n",
    "            )\n",
    "        ],\n",
    "        style = {\n",
    "            'display': 'inline-block',\n",
    "            'width': '100%',\n",
    "            'vertical-align': 'top'\n",
    "        }),\n",
    "        html.Div([\n",
    "            html.H5([\"Total Trips Per Week (2014-2017)\"]),\n",
    "            dcc.Graph(id = 'all_time_week_bar',\n",
    "                config = {'responsive': True, 'modeBarButtonsToRemove': ['zoom2d', 'zoomIn2d', 'zoomOut2d']}\n",
    "            )\n",
    "        ],\n",
    "        style = {\n",
    "            'display': 'inline-block',\n",
    "            'width': '100%'\n",
    "        })\n",
    "    ],\n",
    "    style = {\n",
    "        'width': 'calc(80% - 6em)',\n",
    "        'height': 'auto',\n",
    "        'margin-left': 'calc(15% + 6em)',\n",
    "        'padding-top': '2em',\n",
    "        'display': 'inline-block',\n",
    "        'vertical-align': 'top',\n",
    "        'color': '#aabacc'\n",
    "    })\n",
    "],\n",
    "style = {\n",
    "    'position': 'relative',\n",
    "    'border-bottom': '2px solid #aabacc'\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57aa0596-ff82-454f-a7e2-37d9a715d63e",
   "metadata": {},
   "source": [
    "## Define Function to Generate Plots with Plotly Express\n",
    "Next lets define the functions to build our two charts and link them to our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80aef26c-d1bb-4008-9ad2-bc63544f7f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geospatial bubble chart based on Page Rank and Trip data\n",
    "def get_pagerank_plot(data):\n",
    "    df = calculate_page_rank(data).to_pandas()\n",
    "    g = px.scatter_mapbox(df, lat=\"lat\", lon=\"lon\", color=\"pagerank\", size=\"total_trips\",\n",
    "                          hover_data=[\"station_name\"], mapbox_style=\"carto-positron\",\n",
    "                          color_continuous_scale=px.colors.cyclical.Edge_r, size_max=15, zoom=10,\n",
    "                          height=700\n",
    "                         )\n",
    "    g.layout['uirevision'] = True\n",
    "    return g\n",
    "\n",
    "# Bar chart based on total trips over weeks\n",
    "def get_week_bar_chart(data):\n",
    "    all_time_week_df = data.groupby('all_time_week').size().reset_index()\n",
    "    all_time_week_df.columns = ['week', 'trips']\n",
    "    g = px.bar(all_time_week_df.to_pandas(), \n",
    "               x=\"week\", y='trips', template=dict(layout={'selectdirection': 'h',}), \n",
    "               height=300\n",
    "              )\n",
    "    g.layout['dragmode']='select'\n",
    "    g.layout['uirevision'] = True\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b308ff-d63b-4bf9-9e2d-410749c31a62",
   "metadata": {},
   "source": [
    "## Define Function to Calculate Page Rank\n",
    "Because Plotly Dash applications are hosted through a python backend, the web based charts are able to call custom python functions. Lets use this feature and the speed of cuGraph to calculate new PageRank scores base on a user's selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7acb58-29be-483d-85eb-9ffac18181db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_page_rank(data):\n",
    "    G = cugraph.Graph()\n",
    "    G.from_cudf_edgelist(data, source='from_station_id', destination='to_station_id')\n",
    "    data_page = cugraph.pagerank(G)\n",
    "    return data_page.merge(stations, left_on='vertex', right_on='station_id').drop(columns=['vertex'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681b1285-e514-4662-938a-a46df3899e41",
   "metadata": {},
   "source": [
    "## Define Events and Callbacks\n",
    "Here we define what happens when a user interacts with chart selections through [Dash callbacks](https://dash.plotly.com/basic-callbacks):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a82e1c-9987-4d98-b8fc-2f4538770b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bar_selection_to_query(selection, column):\n",
    "    \"\"\"\n",
    "    Compute pandas query expression string for selection callback data\n",
    "    Args:\n",
    "        selection: selectedData dictionary from Dash callback on a bar trace\n",
    "        column: Name of the column that the selected bar chart is based on\n",
    "    Returns:\n",
    "        String containing a query expression compatible with DataFrame.query. This\n",
    "        expression will filter the input DataFrame to contain only those rows that\n",
    "        are contained in the selection.\n",
    "    \"\"\"\n",
    "    point_inds = [p['label'] for p in selection['points']]\n",
    "    xmin = min(point_inds)  # bin_edges[min(point_inds)]\n",
    "    xmax = max(point_inds) + 1  # bin_edges[max(point_inds) + 1]\n",
    "    xmin_op = \"<=\"\n",
    "    xmax_op = \"<=\"\n",
    "    return f\"{xmin} {xmin_op} {column} and {column} {xmax_op} {xmax}\"\n",
    "\n",
    "# Define callback to update graph, id ties plot code to layout\n",
    "@app.callback(\n",
    "    [\n",
    "        Output('pagerank_plot', 'figure'),\n",
    "        Output('all_time_week_bar', 'figure'),\n",
    "        Output('number', 'figure')\n",
    "    ],\n",
    "    [\n",
    "        Input(\"day\", \"value\"), Input(\"time\", \"value\"),\n",
    "        Input(\"all_time_week_bar\", \"selectedData\")\n",
    "    ]\n",
    ")\n",
    "def update_figure(day, time, selected_weeks):\n",
    "    query = ['day_type == '+str(day) if day != \"\" else \"\", 'time_of_day =='+str(time) if time != \"\" else \"\"]\n",
    "    query_str = ' and '.join([x for x in query if x != \"\"])\n",
    "    \n",
    "    data = trips\n",
    "    if len(query_str) > 0:\n",
    "        data = trips.query(query_str)\n",
    "\n",
    "    week_bar_chart = get_week_bar_chart(data)\n",
    "    \n",
    "    if selected_weeks is not None:\n",
    "        query.append(bar_selection_to_query(selected_weeks, 'all_time_week'))\n",
    "        query_str = ' and '.join([x for x in query if x != \"\"])\n",
    "        if len(query) > 0:\n",
    "            data = trips.query(query_str)\n",
    "    \n",
    "    pagerank_plot = get_pagerank_plot(data)\n",
    "    \n",
    "    number = go.Figure(go.Indicator(\n",
    "                mode=\"number\",\n",
    "                value=data.shape[0]\n",
    "            ))\n",
    "\n",
    "    return pagerank_plot, week_bar_chart, number"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cf3b59-bb46-4218-a960-404e1b47abe9",
   "metadata": {},
   "source": [
    "## Start the Plotly Dash Visualization\n",
    "Now that we have defined everything, lets run the application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4f2491-e05e-43a2-8df3-bb86f7118ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: If you are running in a JupyterHub environment, run the below command:\n",
    "# JupyterDash.infer_jupyter_proxy_config()\n",
    "\n",
    "# NOTE: For Jupyterlab run: \n",
    "# app.run_server(mode=\"jupyterlab\")\n",
    "\n",
    "# NOTE: To run inline with a notebook (NOT recommended): \n",
    "# app.run_server(mode=\"inline\")\n",
    "\n",
    "# NOTE: To run as seperate tab run then click on the link (recommended):\n",
    "app.run_server(debug=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c537c6b1-af3c-4e98-aac3-b93747dffc90",
   "metadata": {},
   "source": [
    "## Final Plotly Dash Visualization\n",
    "This is what the dashboard should look like:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/jupytercon/2020-exactlyallan/master/images/PlotlyDash-Dashboard.png\">\n",
    "          \n",
    "Overall the speed of cuGraph's PageRank as well as the simple interactions make this dashboard intuitive and usable for quickly finding the most important bike stations. With preset filters, it succinctly gives an overview of how this complicated network behaves over time and is able to handle new data seamlessly (not bad for a tutorial app).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8496f52-5d03-4aa6-b8ce-df59453f4a52",
   "metadata": {},
   "source": [
    "### Try It Now\n",
    "See if you can adjust the layout of the above app by reordering the `div` tags and changing the `style` tag values.\n",
    "\n",
    "Or try changing the `css` by adding a reference to `external_stylesheets` as shown below. You can use the external `css` files from example GitHub repos from their [Dash Gallery](https://dash-gallery.plotly.host/Portal/). \n",
    "\n",
    "\n",
    "```\n",
    "external_stylesheets = ['https://codepen.io/chriddyp/pen/bWLwgP.css']\n",
    "\n",
    "app = dash.Dash(__name__, external_stylesheets=external_stylesheets)\n",
    "```\n",
    "\n",
    "NOTE: you'll have to re-run all of the Plotly related cells after updating. Be forewarned, editing css in a notebook is a lot like [this](http://gph.is/1heneJM)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb5ecc5-6f91-4a0f-8787-e25af2744a9b",
   "metadata": {},
   "source": [
    "## A Final Summary on the Benefits of <br> Running with RAPIDS\n",
    "\n",
    "Hopefully as you've clicked through these tutorial notebooks, you've noticed how seamless it is working within the RAPIDS libraries and with other libraries. One of the key goals of RAPIDS is to keep the tools and workflows you are familiar with, but turn them into end-to-end GPU accelerated pipelines. From ETL, exploration, analytics, and visualization - you can take advantage of the speed ups from GPUs.\n",
    "\n",
    "We on the viz team are continuing to integrate with other visualization libraries, and have projects in the works to improve the performance and capabilities of web visualizations even further.\n",
    "\n",
    "RAPIDS is still a relatively young project (we aren't even to 1.0 yet!), but we continue to work towards building out more features and improving. Stay up to date with our projects on our [Home](https://rapids.ai/), [GitHub](https://github.com/rapidsai), and [Twitter page](https://twitter.com/rapidsai)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71368eb1-c13f-4f0d-bdbd-d1e878f26165",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run this cell to show this section's walkthrough video ##\n",
    "from IPython.display import HTML\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/deGQdljxYlY\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72764c0-7a69-4991-916c-56bfedafda0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
